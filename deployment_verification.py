#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éƒ¨ç½²åŠŸèƒ½éªŒè¯è„šæœ¬
éªŒè¯éƒ¨ç½²çš„ç½‘ç«™æ˜¯å¦æ»¡è¶³æ‰€æœ‰è¦æ±‚ï¼š
1. æ­£å¸¸éƒ¨ç½²åˆ°vercel
2. é¦–é¡µæ–°é—»æœ‰tabåˆ†ç±»
3. é¦–é¡µæ–°é—»å¡ç‰‡ï¼Œä¸­æ–‡æ ‡é¢˜ä¸æ‘˜è¦ï¼Œç¡®åˆ‡çš„åª’ä½“ä¸æ—¶é—´
4. è¯¦æƒ…æ˜¯ä¸­æ–‡æ ‡é¢˜ä¸ä¸­æ–‡æ­£æ–‡ï¼ŒåŒ…å«AIç‚¹è¯„ï¼Œåº•éƒ¨æ˜¯ç‚¹å‡»é˜…è¯»åŸæ–‡è·³è½¬æ–°é—»æºé“¾æ¥
"""

import requests
import json
from bs4 import BeautifulSoup
from datetime import datetime
import re

class DeploymentVerifier:
    def __init__(self, deployment_url):
        self.deployment_url = deployment_url
        self.verification_results = {
            "deployment_status": False,
            "tab_categories": False,
            "chinese_content": False,
            "news_cards": False,
            "detail_pages": False,
            "ai_commentary": False,
            "source_links": False,
            "errors": [],
            "details": {}
        }
    
    def log_result(self, test_name, success, details=""):
        """è®°å½•éªŒè¯ç»“æœ"""
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if details:
            print(f"  è¯¦æƒ…: {details}")
        if not success:
            self.verification_results["errors"].append(f"{test_name}: {details}")
    
    def verify_deployment_status(self):
        """éªŒè¯éƒ¨ç½²çŠ¶æ€"""
        try:
            response = requests.get(self.deployment_url, timeout=10)
            if response.status_code == 200:
                self.verification_results["deployment_status"] = True
                self.log_result("1. Verceléƒ¨ç½²çŠ¶æ€", True, "ç½‘ç«™å¯æ­£å¸¸è®¿é—®")
                return response.text
            else:
                self.log_result("1. Verceléƒ¨ç½²çŠ¶æ€", False, f"HTTPçŠ¶æ€ç : {response.status_code}")
                return None
        except Exception as e:
            self.log_result("1. Verceléƒ¨ç½²çŠ¶æ€", False, f"è®¿é—®é”™è¯¯: {str(e)}")
            return None
    
    def verify_tab_categories(self, html_content):
        """éªŒè¯tabåˆ†ç±»åŠŸèƒ½"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æŸ¥æ‰¾tabç›¸å…³å…ƒç´ 
        tabs = soup.find_all(['div', 'ul', 'nav'], class_=re.compile(r'tab|category|filter', re.I))
        tab_buttons = soup.find_all(['button', 'a'], class_=re.compile(r'tab|category', re.I))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†ç±»æ ‡ç­¾
        categories_found = []
        for element in soup.find_all(text=re.compile(r'AIç§‘æŠ€|ç§‘æŠ€åˆ›æ–°|ç»æµæ–°é—»|æ¸¸æˆèµ„è®¯')):
            categories_found.append(element.strip())
        
        if tabs or tab_buttons or categories_found:
            self.verification_results["tab_categories"] = True
            self.verification_results["details"]["categories"] = list(set(categories_found))
            self.log_result("2. Tabåˆ†ç±»åŠŸèƒ½", True, f"å‘ç°åˆ†ç±»: {', '.join(set(categories_found))}")
        else:
            self.log_result("2. Tabåˆ†ç±»åŠŸèƒ½", False, "æœªå‘ç°tabåˆ†ç±»å…ƒç´ ")
    
    def verify_chinese_content(self, html_content):
        """éªŒè¯ä¸­æ–‡å†…å®¹"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æ£€æŸ¥ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
        text_content = soup.get_text()
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text_content))
        total_chars = len(text_content.replace(' ', '').replace('\n', ''))
        
        if total_chars > 0:
            chinese_ratio = chinese_chars / total_chars
            if chinese_ratio > 0.3:  # ä¸­æ–‡å­—ç¬¦å æ¯”è¶…è¿‡30%
                self.verification_results["chinese_content"] = True
                self.log_result("3. ä¸­æ–‡å†…å®¹", True, f"ä¸­æ–‡å­—ç¬¦å æ¯”: {chinese_ratio:.1%}")
            else:
                self.log_result("3. ä¸­æ–‡å†…å®¹", False, f"ä¸­æ–‡å­—ç¬¦å æ¯”è¿‡ä½: {chinese_ratio:.1%}")
        else:
            self.log_result("3. ä¸­æ–‡å†…å®¹", False, "æ— æ³•è·å–æ–‡æœ¬å†…å®¹")
    
    def verify_news_cards(self, html_content):
        """éªŒè¯æ–°é—»å¡ç‰‡åŠŸèƒ½"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æŸ¥æ‰¾æ–°é—»å¡ç‰‡
        news_items = soup.find_all(['div', 'article'], class_=re.compile(r'news|item|card', re.I))
        
        if not news_items:
            news_items = soup.find_all('div', class_='news-item')
        
        news_count = len(news_items)
        
        if news_count > 0:
            self.verification_results["news_cards"] = True
            
            # æ£€æŸ¥æ–°é—»å¡ç‰‡å†…å®¹
            sample_item = news_items[0] if news_items else None
            details = []
            
            if sample_item:
                # æ£€æŸ¥æ ‡é¢˜
                title_elem = sample_item.find(['h1', 'h2', 'h3', 'div'], class_=re.compile(r'title', re.I))
                if title_elem:
                    details.append(f"æ ‡é¢˜: {title_elem.get_text()[:50]}...")
                
                # æ£€æŸ¥æ—¶é—´
                time_elem = sample_item.find(['div', 'span'], class_=re.compile(r'time|date|meta', re.I))
                if time_elem:
                    details.append(f"æ—¶é—´ä¿¡æ¯: {time_elem.get_text()[:30]}")
                
                # æ£€æŸ¥æ‘˜è¦
                desc_elem = sample_item.find(['div', 'p'], class_=re.compile(r'desc|summary|content', re.I))
                if desc_elem:
                    details.append(f"æ‘˜è¦: {desc_elem.get_text()[:50]}...")
            
            self.verification_results["details"]["news_cards"] = {
                "count": news_count,
                "sample_content": details
            }
            
            self.log_result("4. æ–°é—»å¡ç‰‡", True, f"å‘ç° {news_count} ä¸ªæ–°é—»é¡¹")
        else:
            self.log_result("4. æ–°é—»å¡ç‰‡", False, "æœªå‘ç°æ–°é—»å¡ç‰‡")
    
    def verify_detail_pages(self):
        """éªŒè¯è¯¦æƒ…é¡µåŠŸèƒ½"""
        try:
            # å°è¯•è®¿é—®ä¸€ä¸ªè¯¦æƒ…é¡µ
            detail_url = f"{self.deployment_url}/news/ai_0_1753614821.html"
            response = requests.get(detail_url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æ£€æŸ¥ä¸­æ–‡æ ‡é¢˜
                title_elem = soup.find(['h1', 'h2', 'title'])
                has_chinese_title = False
                if title_elem:
                    title_text = title_elem.get_text()
                    has_chinese_title = bool(re.search(r'[\u4e00-\u9fff]', title_text))
                
                # æ£€æŸ¥AIç‚¹è¯„
                ai_commentary = soup.find(['div', 'section'], class_=re.compile(r'ai|commentary|comment', re.I))
                has_ai_commentary = ai_commentary is not None
                
                # æ£€æŸ¥åŸæ–‡é“¾æ¥
                source_links = soup.find_all('a', href=True)
                has_source_link = any('http' in link.get('href', '') for link in source_links)
                
                if has_chinese_title and has_ai_commentary and has_source_link:
                    self.verification_results["detail_pages"] = True
                    self.verification_results["ai_commentary"] = has_ai_commentary
                    self.verification_results["source_links"] = has_source_link
                    
                    self.log_result("5. è¯¦æƒ…é¡µåŠŸèƒ½", True, "åŒ…å«ä¸­æ–‡æ ‡é¢˜ã€AIç‚¹è¯„å’ŒåŸæ–‡é“¾æ¥")
                    self.log_result("6. AIç‚¹è¯„åŠŸèƒ½", True, "è¯¦æƒ…é¡µåŒ…å«AIç‚¹è¯„")
                    self.log_result("7. åŸæ–‡é“¾æ¥", True, "åŒ…å«è·³è½¬é“¾æ¥")
                else:
                    missing = []
                    if not has_chinese_title: missing.append("ä¸­æ–‡æ ‡é¢˜")
                    if not has_ai_commentary: missing.append("AIç‚¹è¯„")
                    if not has_source_link: missing.append("åŸæ–‡é“¾æ¥")
                    
                    self.log_result("5. è¯¦æƒ…é¡µåŠŸèƒ½", False, f"ç¼ºå°‘: {', '.join(missing)}")
                    self.log_result("6. AIç‚¹è¯„åŠŸèƒ½", has_ai_commentary, "")
                    self.log_result("7. åŸæ–‡é“¾æ¥", has_source_link, "")
            else:
                self.log_result("5. è¯¦æƒ…é¡µåŠŸèƒ½", False, f"è¯¦æƒ…é¡µè®¿é—®å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            self.log_result("5. è¯¦æƒ…é¡µåŠŸèƒ½", False, f"è®¿é—®è¯¦æƒ…é¡µæ—¶å‡ºé”™: {str(e)}")
    
    def run_full_verification(self):
        """è¿è¡Œå®Œæ•´éªŒè¯"""
        print("=" * 60)
        print(f"å¼€å§‹éªŒè¯éƒ¨ç½²: {self.deployment_url}")
        print("=" * 60)
        
        # 1. éªŒè¯éƒ¨ç½²çŠ¶æ€
        html_content = self.verify_deployment_status()
        
        if html_content:
            # 2. éªŒè¯tabåˆ†ç±»
            self.verify_tab_categories(html_content)
            
            # 3. éªŒè¯ä¸­æ–‡å†…å®¹
            self.verify_chinese_content(html_content)
            
            # 4. éªŒè¯æ–°é—»å¡ç‰‡
            self.verify_news_cards(html_content)
        
        # 5-7. éªŒè¯è¯¦æƒ…é¡µåŠŸèƒ½
        self.verify_detail_pages()
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report()
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("éªŒè¯ç»“æœæ€»ç»“")
        print("=" * 60)
        
        total_tests = 7
        passed_tests = sum([
            self.verification_results["deployment_status"],
            self.verification_results["tab_categories"],
            self.verification_results["chinese_content"],
            self.verification_results["news_cards"],
            self.verification_results["detail_pages"],
            self.verification_results["ai_commentary"],
            self.verification_results["source_links"]
        ])
        
        print(f"é€šè¿‡æµ‹è¯•: {passed_tests}/{total_tests}")
        print(f"æˆåŠŸç‡: {passed_tests/total_tests:.1%}")
        
        if self.verification_results["errors"]:
            print("\nâŒ å‘ç°çš„é—®é¢˜:")
            for error in self.verification_results["errors"]:
                print(f"  - {error}")
        
        if passed_tests == total_tests:
            print("\nğŸ‰ æ‰€æœ‰åŠŸèƒ½éªŒè¯é€šè¿‡ï¼éƒ¨ç½²æˆåŠŸï¼")
        else:
            print(f"\nâš ï¸  è¿˜æœ‰ {total_tests - passed_tests} ä¸ªåŠŸèƒ½éœ€è¦ä¿®å¤")
        
        # ä¿å­˜éªŒè¯æŠ¥å‘Š
        report_file = f"verification_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.verification_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        print(f"ğŸŒ éƒ¨ç½²åœ°å€: {self.deployment_url}")

def main():
    deployment_url = "https://docs-n68d4n5f4-velists-projects.vercel.app"
    verifier = DeploymentVerifier(deployment_url)
    verifier.run_full_verification()

if __name__ == "__main__":
    main()