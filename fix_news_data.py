#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤æ–°é—»æ•°æ®æ—¶é—´å­—æ®µ
"""

import json
from datetime import datetime, timezone, timedelta

def format_beijing_time(iso_time_str):
    """å°†ISOæ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´å¹¶æ ¼å¼åŒ–æ˜¾ç¤º"""
    try:
        if not iso_time_str:
            return "æœªçŸ¥æ—¶é—´"
            
        if iso_time_str.endswith('Z'):
            dt = datetime.fromisoformat(iso_time_str[:-1] + '+00:00')
        else:
            dt = datetime.fromisoformat(iso_time_str)
        
        beijing_tz = timezone(timedelta(hours=8))
        beijing_time = dt.astimezone(beijing_tz)
        now_beijing = datetime.now(beijing_tz)
        time_diff = now_beijing - beijing_time
        
        if time_diff.days > 0:
            return f"{time_diff.days}å¤©å‰"
        elif time_diff.seconds > 3600:
            hours = time_diff.seconds // 3600
            return f"{hours}å°æ—¶å‰"
        elif time_diff.seconds > 60:
            minutes = time_diff.seconds // 60
            return f"{minutes}åˆ†é’Ÿå‰"
        else:
            return "åˆšåˆš"
    except:
        return "æœªçŸ¥æ—¶é—´"

def categorize_article(title, summary):
    """æ™ºèƒ½åˆ†ç±»æ–‡ç« """
    title_lower = title.lower()
    summary_lower = summary.lower()
    combined = (title_lower + " " + summary_lower)
    
    # æ¨¡å‹ç›¸å…³å…³é”®è¯
    model_keywords = [
        'gpt', 'chatgpt', 'gemini', 'claude', 'llama', 'qwen', 'baichuan', 'chatglm',
        'model', 'llm', 'large language model', 'ai model', 'neural network',
        'transformer', 'bert', 'dall-e', 'midjourney', 'stable diffusion'
    ]
    
    if any(word in combined for word in model_keywords):
        return 'æ¨¡å‹', 'ğŸ¤–'
    elif any(word in combined for word in ['funding', 'investment', 'raise', 'billion', 'million', 'valuation']):
        return 'æŠ•èµ„å¹¶è´­', 'ğŸ’°'
    elif any(word in combined for word in ['openai', 'google', 'microsoft', 'apple', 'meta', 'amazon', 'nvidia']):
        return 'å…¬å¸åŠ¨æ€', 'ğŸ¢'
    elif any(word in combined for word in ['breakthrough', 'research', 'algorithm', 'technology', 'innovation']):
        return 'æŠ€æœ¯çªç ´', 'ğŸš€'
    elif any(word in combined for word in ['regulation', 'policy', 'government', 'law', 'compliance']):
        return 'æ”¿ç­–ç›‘ç®¡', 'âš–ï¸'
    elif any(word in combined for word in ['application', 'healthcare', 'finance', 'education', 'automotive']):
        return 'è¡Œä¸šåº”ç”¨', 'ğŸ¯'
    
    return 'çƒ­é—¨', 'ğŸ”¥'

# è¯»å–å¹¶å¤„ç†æ•°æ®
with open('docs/enhanced_news_data.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

articles = data['articles']

# ä¸ºæ¯ç¯‡æ–‡ç« æ·»åŠ ç¼ºå¤±çš„å­—æ®µ
for article in articles:
    if 'time' not in article:
        article['time'] = format_beijing_time(article.get('publishedAt', ''))
    
    if 'category' not in article or 'category_icon' not in article:
        category, category_icon = categorize_article(article.get('title', ''), article.get('summary', ''))
        article['category'] = category
        article['category_icon'] = category_icon

# ä¿å­˜æ›´æ–°åçš„æ•°æ®
data['articles'] = articles
with open('docs/enhanced_news_data.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

print(f"âœ… å·²ä¿®å¤ {len(articles)} æ¡æ–°é—»çš„å­—æ®µ")
print("ğŸ“Š åˆ†ç±»ç»Ÿè®¡:")

category_count = {}
for article in articles:
    cat = article.get('category', 'æœªåˆ†ç±»')
    category_count[cat] = category_count.get(cat, 0) + 1

for cat, count in category_count.items():
    print(f"  {cat}: {count} æ¡")