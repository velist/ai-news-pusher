#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®‰å…¨çš„æ–°é—»æ•°é‡å¢åŠ è„šæœ¬ - ä¿æŒä¸­æ–‡æ˜¾ç¤º
"""

import os
import json
import urllib.request
import urllib.parse
from datetime import datetime, timezone, timedelta
import hashlib
import time

def load_env_file():
    """åŠ è½½ç¯å¢ƒå˜é‡"""
    env_path = '.env'
    if os.path.exists(env_path):
        try:
            with open(env_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        os.environ[key.strip()] = value.strip()
            return True
        except Exception as e:
            print(f"ç¯å¢ƒå˜é‡åŠ è½½å¤±è´¥: {e}")
            return False
    return False

def get_more_gnews():
    """ä»GNewsè·å–æ›´å¤šæ–°é—»"""
    load_env_file()
    gnews_key = os.environ.get('GNEWS_API_KEY')
    
    if not gnews_key:
        print("âŒ ç¼ºå°‘GNews APIå¯†é’¥")
        return []
    
    all_articles = []
    queries = [
        "AI OR artificial intelligence",
        "OpenAI OR ChatGPT OR GPT", 
        "Google AI OR Gemini",
        "machine learning OR deep learning",
        "AI startup OR AI funding",
        "AI research OR AI breakthrough"
    ]
    
    print("ğŸ“¡ ä»GNewsè·å–æ›´å¤šæ–°é—»...")
    
    for i, query in enumerate(queries):
        try:
            print(f"  æŸ¥è¯¢ {i+1}: {query}")
            url = f"https://gnews.io/api/v4/search?q={urllib.parse.quote(query)}&lang=en&max=8&apikey={gnews_key}"
            
            with urllib.request.urlopen(url, timeout=30) as response:
                data = json.loads(response.read().decode('utf-8'))
            
            if 'articles' in data:
                for article in data['articles']:
                    all_articles.append({
                        'title': article.get('title', ''),
                        'summary': article.get('description', ''),
                        'url': article.get('url', ''),
                        'source': article.get('source', {}).get('name', 'æœªçŸ¥æ¥æº'),
                        'publishedAt': article.get('publishedAt', ''),
                        'image': article.get('image', ''),
                        'api_source': 'gnews'
                    })
            
            time.sleep(2)  # é¿å…APIé™åˆ¶
        except Exception as e:
            print(f"    é”™è¯¯: {e}")
    
    # å»é‡
    seen_titles = set()
    unique_articles = []
    
    for article in all_articles:
        if article['title'] and article['summary']:
            title_hash = hashlib.md5(article['title'].encode()).hexdigest()[:12]
            if title_hash not in seen_titles:
                seen_titles.add(title_hash)
                article['id'] = title_hash
                unique_articles.append(article)
    
    print(f"âœ… è·å–åˆ° {len(unique_articles)} æ¡æ–°é—»")
    return unique_articles

def update_news_data():
    """æ›´æ–°æ–°é—»æ•°æ®"""
    # è·å–æ–°æ–°é—»
    new_articles = get_more_gnews()
    
    # è¯»å–ç°æœ‰æ•°æ®
    try:
        with open('docs/enhanced_news_data.json', 'r', encoding='utf-8') as f:
            existing_data = json.load(f)
        existing_articles = existing_data.get('articles', [])
    except:
        existing_articles = []
    
    # åˆå¹¶æ•°æ®
    all_articles = new_articles + existing_articles
    
    # å»é‡
    seen_ids = set()
    final_articles = []
    
    for article in all_articles:
        article_id = article.get('id')
        if article_id and article_id not in seen_ids:
            seen_ids.add(article_id)
            final_articles.append(article)
    
    # é™åˆ¶åˆ°50æ¡
    final_articles = final_articles[:50]
    
    print(f"ğŸ‰ æœ€ç»ˆæ•°æ®: {len(final_articles)} æ¡æ–°é—»")
    
    # ä¿å­˜æ•°æ®
    updated_data = {
        'last_updated': datetime.now().isoformat(),
        'total_count': len(final_articles),
        'categories': list(set(article.get('category', 'çƒ­é—¨') for article in final_articles)),
        'articles': final_articles
    }
    
    with open('docs/enhanced_news_data.json', 'w', encoding='utf-8') as f:
        json.dump(updated_data, f, ensure_ascii=False, indent=2)
    
    print("âœ… æ–°é—»æ•°æ®å·²æ›´æ–°")
    return final_articles

if __name__ == "__main__":
    print("ğŸš€ å®‰å…¨å¢åŠ æ–°é—»æ•°é‡...")
    articles = update_news_data()